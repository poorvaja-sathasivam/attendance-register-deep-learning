{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "09b75028-265a-4490-b2fe-8badbd0b3fcb",
   "metadata": {},
   "source": [
    "# Build a deep-learning classifer that recognises students in the classroom from close up facial images to register their attendance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bb3db615-1aea-4b9e-a830-963b013d8be5",
   "metadata": {},
   "source": [
    "## 1. Install all the necessary libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f1110c-6515-4077-8293-0e6cca47a4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import cv2\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "from collections import OrderedDict\n",
    "import keras \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from PIL import Image\n",
    "path_dataset = \"Dataset/lfw-deepfunneled/lfw-deepfunneled\"\n",
    "\n",
    "%pip install mtcnn\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "\n",
    "import shutil\n",
    "from shutil import unpack_archive\n",
    "from subprocess import check_output\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0393a57a-fd9f-45aa-9bd7-92af588bd9be",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "86cebfca-0dee-4604-b47f-3a310cb63d6a",
   "metadata": {},
   "source": [
    "### 2.1 Loading and reading all the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9470dd-590a-428c-8e99-c0893df669d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Loading all the necessary data \n",
    "lfw_allnames = pd.read_csv(\"Dataset/lfw_allnames.csv\")\n",
    "matchpairsDevTest = pd.read_csv(\"Dataset/matchpairsDevTest.csv\")\n",
    "matchpairsDevTrain = pd.read_csv(\"Dataset/matchpairsDevTrain.csv\")\n",
    "mismatchpairsDevTest = pd.read_csv(\"Dataset/mismatchpairsDevTest.csv\")\n",
    "mismatchpairsDevTrain = pd.read_csv(\"Dataset/mismatchpairsDevTrain.csv\")\n",
    "pairs = pd.read_csv(\"Dataset/pairs.csv\")\n",
    "people = pd.read_csv(\"Dataset/people.csv\")\n",
    "peopleDevTest = pd.read_csv(\"Dataset/peopleDevTest.csv\")\n",
    "peoplleDevTrain = pd.read_csv(\"Dataset/peopleDevTrain.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "675a136b-c484-4f7d-ba13-d52f37c5d04d",
   "metadata": {},
   "source": [
    "### 2.2 Initial Exploration of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bfdd1e-5003-4a7d-904a-af4743db0737",
   "metadata": {},
   "outputs": [],
   "source": [
    "lfw_allnames.head()\n",
    "matchpairsDevTest.head() \n",
    "matchpairsDevTrain.head() \n",
    "mismatchpairsDevTest.head() \n",
    "mismatchpairsDevTrain.head() \n",
    "pairs.head() \n",
    "people.head() \n",
    "peopleDevTest.head()\n",
    "peoplleDevTrain.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9da310f9-5ea2-4041-907f-810050fd286e",
   "metadata": {},
   "source": [
    "### 2.3 Exploring the initial statistics and distributions of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a79ec98-7310-4841-8aac-c484750c5f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lfw_allnames.describe()\n",
    "# lfw_allnames.hist()\n",
    "# people.describe()\n",
    "# people.hist()\n",
    "person_images = lfw_allnames.groupby('name')['images'].sum().reset_index()\n",
    "person_images_sorted_desc = person_images.sort_values('cimages', ascending=False).head(10)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(person_images_sorted_desc['name'], person_images_sorted_desc['images'])\n",
    "plt.xlabel('Person')\n",
    "plt.ylabel('Number of Images')\n",
    "plt.title('Number of images per person in Descending Order (Top 10)')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fc5ab309-e0c1-429e-a6bb-9779d530ddf5",
   "metadata": {},
   "source": [
    "### 2.4 A general overview of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce6800d-311f-473c-aa91-78dbc75f9809",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_persons = lfw_allnames.shape[0]\n",
    "has_multiple_images = sum(lfw_allnames.images > 1)\n",
    "total_num_images = sum(lfw_allnames.images)\n",
    "has_most_images = lfw_allnames.iloc[lfw_allnames['images'].idxmax()][0]\n",
    "unique_images = max(lfw_allnames.images)\n",
    "\n",
    "# Printing the information \n",
    "print(\"Important to note:\")\n",
    "print(\"\\n\")\n",
    "print(\"1. The dataset has a total of \"+str(total_num_images)+  \" images. \")\n",
    "print(\"2. In which there are a total of \" +str(unique_persons)+ \" unique names of people.\")\n",
    "print(\"3. \"+str(has_multiple_images)+  \" of people in the dataset have multiple images. \")\n",
    "print(\"4. The person who has the most number of images is: \"+str(has_most_images))\n",
    "print(\"5. There are a total of \"+str(unique_images)+  \" unique images in the dataset. \")\n",
    "print(\"\\n\")\n",
    "\n",
    "# lfw_allnames.describe()\n",
    "# lfw_allnames.hist()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "575de518-ef18-4776-9b20-961318037de5",
   "metadata": {},
   "source": [
    "### 2.5 Cleaning and handling missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e55abaa-f026-4d6c-8b60-ee82e2c85a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning pairs data \n",
    "pairs = pairs.rename(columns ={'name': 'name1', 'Unnamed: 3': 'name2'})\n",
    "matched_pairs = pairs[pairs[\"name2\"].isnull()].drop(\"name2\",axis=1)\n",
    "mismatched_pairs = pairs[pairs[\"name2\"].notnull()]\n",
    "\n",
    "# Handling missing/null values\n",
    "lfw_allnames.dropna(inplace=True)\n",
    "people = people[people.name.notnull()]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "16fe5df4-b770-403f-917f-825447f747a3",
   "metadata": {},
   "source": [
    "### 2.6 Organising data and splitting them into train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75884364-2234-40b8-95a5-df27f06b68ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dataframe has the 'name' column for the person's name and the 'path_of_image' column for the corresponding image file path.\n",
    "path_of_image = lfw_allnames.loc[lfw_allnames.index.repeat(lfw_allnames['images'])]\n",
    "# counting the number of images in each group \n",
    "path_of_image['image_path'] = 1 + path_of_image.groupby('name').cumcount()\n",
    "# Formatting the file to start with 0 and having a max of 4 characters\n",
    "path_of_image['image_path'] = path_of_image.image_path.apply(lambda x: '{0:0>4}'.format(x))\n",
    "path_of_image['image_path'] = path_of_image.name + \"/\" + path_of_image.name + \"_\" + path_of_image.image_path + \".jpg\"\n",
    "path_of_image = path_of_image.drop(\"images\",1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6a8e86-4c9c-4d0a-91e2-34bda34eb4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(path_of_image, test_size=0.2)\n",
    "train_data = train_data.reset_index().drop(\"index\",1)\n",
    "test_data = test_data.reset_index().drop(\"index\",1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cf4023-3b66-4359-854a-117c591dc71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_individuals = set(train_data['name'])\n",
    "test_individuals = set(test_data['name'])\n",
    "common_individuals = train_individuals.intersection(test_individuals)\n",
    "if len(common_individuals) > 0:\n",
    "    print(\"There are common individuals in both the training and test set.\")\n",
    "else:\n",
    "    print(\"There are no common individuals in the training and test set.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332014ca-778b-477e-a3c3-2f5a4053be89",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_width = None\n",
    "expected_height = None\n",
    "files = path_of_image.image_path\n",
    "\n",
    "# Iterate over each image file path in the dataframe\n",
    "for file in files:\n",
    "    im_path = \"Dataset/lfw-deepfunneled/lfw-deepfunneled/\" + str(file)\n",
    "    # Open the image using PIL library\n",
    "    image = Image.open(im_path)\n",
    "    width, height = image.size\n",
    "    \n",
    "    # Set the expected resolution if it's the first iteration\n",
    "    if expected_width is None or expected_height is None:\n",
    "        expected_width = width\n",
    "        expected_height = height\n",
    "    \n",
    "    # Compare the resolution of the current image with the expected resolution\n",
    "    if width != expected_width or height != expected_height:\n",
    "        print(f\"Inconsistent resolution found in image: {image_path}\")\n",
    "        print(f\"Expected resolution: {expected_width}x{expected_height}\")\n",
    "        print(f\"Actual resolution: {width}x{height}\")\n",
    "        break\n",
    "else:\n",
    "    print(\"All images have consistent resolution.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf4f741-24b4-4816-9a7f-b57b9a6826f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_of_image['name'].value_counts()[:10].plot(kind = \"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f239822c-cfc7-499d-9f2b-f2bc8fc54288",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_with_single_img = len(lfw_allnames[lfw_allnames['images'] == 1])\n",
    "print(str(person_with_single_img)+ \" number of people have only one image in their group/class.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798e4485-4d49-430d-8118-677be411cf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "im = Image.open(\"Dataset/lfw-deepfunneled/lfw-deepfunneled/\" + str(train_data.image_path[0]))\n",
    "plt.imshow(im)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3dff38c0-dcdd-494d-9162-08002211258f",
   "metadata": {},
   "source": [
    "## 3. Model "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e55bf97f-601c-4b5b-aa7d-1a0ace225356",
   "metadata": {},
   "source": [
    "### 3.1 Initial model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13608df-ad19-4c9a-bbdf-e5b0c2d5f1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as patches\n",
    "face_detector = MTCNN()\n",
    "image = cv2.imread(\"Dataset/lfw-deepfunneled/lfw-deepfunneled/\" + str(train_data.image_path[0]))\n",
    "result = face_detector.detect_faces(image)\n",
    "\n",
    "# Boundaries\n",
    "box = result[0]['box']\n",
    "keypoints = result[0]['keypoints']\n",
    "\n",
    "# Plot image\n",
    "fig,ax = plt.subplots(1)\n",
    "ax.imshow(image)\n",
    "\n",
    "rect = patches.Rectangle(box[0:2],box[2],box[3],linewidth=1,edgecolor='b',facecolor='none')\n",
    "\n",
    "ax.add_patch(rect)\n",
    "\n",
    "for key in keypoints:\n",
    "    rect_key = patches.Rectangle(keypoints[key],1,1,linewidth=10,edgecolor='r',facecolor='none')\n",
    "    ax.add_patch(rect_key)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c56a2b-d65f-440f-b270-0e379881f906",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_of_image['name'].value_counts()[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c8c336-34e0-446b-9fe9-5c2dd20b7871",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([path_of_image[path_of_image.name==\"George_W_Bush\"].sample(75),\n",
    "                        path_of_image[path_of_image.name==\"Colin_Powell\"].sample(75),\n",
    "                        path_of_image[path_of_image.name==\"Tony_Blair\"].sample(75),\n",
    "                        path_of_image[path_of_image.name==\"Donald_Rumsfeld\"].sample(75),\n",
    "                        path_of_image[path_of_image.name==\"Gerhard_Schroeder\"].sample(75),\n",
    "                        path_of_image[path_of_image.name==\"Ariel_Sharon\"].sample(75)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce976f7c-472a-4889-8f16-d71690e2ac55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move Images to train/test/val folders \n",
    "\n",
    "def directory_mover(data,dir_name):\n",
    "    co = 0\n",
    "    for image in data.image_path:\n",
    "        # create top directory\n",
    "        if not os.path.exists(os.path.join('Core/',dir_name)):\n",
    "            shutil.os.mkdir(os.path.join('Core/',dir_name))\n",
    "        \n",
    "        data_type = data[data['image_path'] == image]['name']\n",
    "        data_type = str(list(data_type)[0])\n",
    "        if not os.path.exists(os.path.join('Core/',dir_name,data_type)):\n",
    "            shutil.os.mkdir(os.path.join('Core/',dir_name,data_type))\n",
    "        path_from = os.path.join('Dataset/lfw-deepfunneled/lfw-deepfunneled/',image)\n",
    "        path_to = os.path.join('Core/',dir_name,data_type)\n",
    "        \n",
    "        shutil.copy(path_from, path_to)\n",
    "        co += 1\n",
    "        \n",
    "    print('Moved {} images to {} folder.'.format(co,dir_name))\n",
    "    \n",
    "#     source_p = source[\"image_path\"].iloc[0] \n",
    "    \n",
    "#     # Making directories for train/test/val\n",
    "#     train_dir = os.path.join(dest, 'train')\n",
    "#     test_dir = os.path.join(dest, 'test')\n",
    "#     val_dir = os.path.join(dest, 'val')\n",
    "#     os.makedirs(train_dir, exist_ok=True)\n",
    "#     os.makedirs(test_dir, exist_ok=True)\n",
    "#     os.makedirs(val_dir, exist_ok=True)\n",
    "    \n",
    "#     #Get image files \n",
    "#     img_files = os.listdir(source_p)\n",
    "#     num_of_images = len(img_files)\n",
    "    \n",
    "#     # Move images to the train directory\n",
    "#     source_path = os.path.join(source, img_files[i])\n",
    "#     destination_path = os.path.join(train_dir, img_files[i])\n",
    "#     shutil.move(source_path, destination_path)\n",
    "    \n",
    "#     # Move images to the test directory\n",
    "#     source_path = os.path.join(source, img_files[i])\n",
    "#     destination_path = os.path.join(test_dir, img_files[i])\n",
    "#     shutil.move(source_path, destination_path)\n",
    "    \n",
    "#     # Move images to the val directory\n",
    "#     source_path = os.path.join(source_dir, img_files[i])\n",
    "#     destination_path = os.path.join(val_dir, img_files[i])\n",
    "#     shutil.move(source_path, destination_path)\n",
    "\n",
    "#     print(\"Images have moved to their respective train/test/val directories successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ffe819-f812-4774-bfb4-5fd745b9b410",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# withhold final test data\n",
    "d_train, d_test = train_test_split(data, test_size=0.2)\n",
    "# split into validation data\n",
    "d_train, d_val = train_test_split(d_train,test_size=0.2)\n",
    "\n",
    "# Model Setup\n",
    "classifier = Sequential()\n",
    "classifier.add(Conv2D(32, (3, 3), input_shape = (250, 250, 3), activation = 'relu'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "classifier.add(Flatten())\n",
    "classifier.add(Dense(units = 128, activation = 'relu'))\n",
    "classifier.add(Dense(units = 6, activation = 'softmax'))\n",
    "classifier.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Move to a seperate directory \n",
    "directory_mover(d_train,\"train/\")\n",
    "directory_mover(d_val,\"val/\")\n",
    "directory_mover(d_test,\"test/\")\n",
    "\n",
    "# Create image data generators\n",
    "train_datagen = keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "    \n",
    "test_datagen = keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "    \n",
    "training_set = train_datagen.flow_from_directory('Core/train/',\n",
    "                                                 target_size = (250, 250),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'categorical')\n",
    "\n",
    "validation_set = test_datagen.flow_from_directory('Core/val',\n",
    "                                            target_size = (250, 250),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode = 'categorical')\n",
    "    \n",
    "testing_set = train_datagen.flow_from_directory('Core/test/',\n",
    "                                                 target_size = (250, 250),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'categorical')\n",
    "history = classifier.fit(training_set, steps_per_epoch = 9, epochs = 20, validation_data = validation_set, validation_steps = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fabc88-8b18-4f4f-a2e6-c896c226a5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d1a538-d6b5-412c-86a6-64c0276b7142",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_test_names = []\n",
    "# collect all file names\n",
    "for i in range(len(testing_set.filenames)):\n",
    "    multi_test_names.append(testing_set.filenames[i])\n",
    "# extract unique names, in order\n",
    "for i in range(len(multi_test_names)):\n",
    "    multi_test_names[i] = multi_test_names[i].split(\"/\")[0]\n",
    "multi_test_name_order = list(OrderedDict.fromkeys(multi_test_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0604c45b-1539-48ac-9b26-1083954d72d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import load_img,img_to_array\n",
    "# create a function to predict class of images in a directory, given a trained classifier\n",
    "def predictions(directory, classifier, binary=False):\n",
    "    predictions = []\n",
    "    class_labels = sorted(os.listdir(directory))\n",
    "    \n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
    "            image_path = os.path.join(directory, filename)\n",
    "            test_image = load_img(image_path, target_size=(250, 250))\n",
    "            test_image = img_to_array(test_image)\n",
    "            test_image = np.expand_dims(test_image, axis=0)\n",
    "            test_image /= 255.0\n",
    "            \n",
    "            if binary:\n",
    "                result = float(str(classifier.predict(test_image))[2])\n",
    "            else:\n",
    "                result = np.argmax(classifier.predict(test_image))\n",
    "            \n",
    "            predicted_class = class_labels[result]\n",
    "            predictions.append((image_path, predicted_class))\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36634593-b387-4aee-82d5-f8450d14a25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "multi_predictions_0 = predictions(\"Core/test/\" + multi_test_name_order[0] + \"/\",classifier,binary=False)\n",
    "multi_predictions_1 = predictions(\"Core/test/\" + multi_test_name_order[1] + \"/\",classifier,binary=False)\n",
    "multi_predictions_2 = predictions(\"Core/test/\" + multi_test_name_order[2] + \"/\",classifier,binary=False)\n",
    "multi_predictions_3 = predictions(\"Core/test/\" + multi_test_name_order[3] + \"/\",classifier,binary=False)\n",
    "multi_predictions_4 = predictions(\"Core/test/\" + multi_test_name_order[4] + \"/\",classifier,binary=False)\n",
    "multi_predictions_5 = predictions(\"Core/test/\" + multi_test_name_order[5] + \"/\",classifier,binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cc2b1f-de70-4d96-a3f9-c0ec5bfc998f",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_predictions_frame = pd.DataFrame(list(zip(multi_predictions_0 + multi_predictions_1 + multi_predictions_2 + multi_predictions_3 + multi_predictions_4 + multi_predictions_5,\n",
    "                                                [0] * len(multi_predictions_0) + [1] * len(multi_predictions_1) + [2] * len(multi_predictions_2) + [3] * len(multi_predictions_3) + [4] * len(multi_predictions_4) + [5] * len(multi_predictions_5))),\n",
    "                                       columns = ['Predictions','Actual'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ac13a5-2611-4a7e-8a00-92e30849baf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prec_acc(predictions_frame):\n",
    "    classes = predictions_frame['Actual'].unique()\n",
    "    precision = []\n",
    "    recall = []\n",
    "    accuracy = []\n",
    "    \n",
    "    for i in classes:\n",
    "        tp = predictions_frame[(predictions_frame['Actual'] == i) & (predictions_frame['Predictions'] == i)].shape[0]\n",
    "        fp = predictions_frame[(predictions_frame['Actual'] != i) & (predictions_frame['Predictions'] == i)].shape[0]\n",
    "        tn = predictions_frame[(predictions_frame['Actual'] != i) & (predictions_frame['Predictions'] != i)].shape[0]\n",
    "        fn = predictions_frame[(predictions_frame['Actual'] == i) & (predictions_frame['Predictions'] != i)].shape[0]\n",
    "        total_preds = predictions_frame.shape[0]\n",
    "        \n",
    "        precision.append(tp / (tp + fp + 1e-10))  # Adding a small value to avoid division by zero\n",
    "        accuracy.append((tp + tn) / total_preds)\n",
    "        recall.append(tp / (tp + fn + 0.5))  # Adding a small value to avoid division by zero\n",
    "    \n",
    "    return precision, accuracy, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78ae377-6b03-435e-ae49-01360144c4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "multi_accuracy = prec_acc(multi_predictions_frame)\n",
    "print('Precision:' + str(multi_accuracy[1]))\n",
    "print('Recall:' + str(multi_accuracy[2]))\n",
    "print(multi_test_name_order)\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48033552-a33b-4f23-8dca-4dd7ab254b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"train\" in os.listdir(\"./\"):\n",
    "    shutil.rmtree(\"./train\")\n",
    "if \"val\" in os.listdir(\"./\"):\n",
    "    shutil.rmtree(\"./val\")\n",
    "if \"test\" in os.listdir(\"./\"):\n",
    "    shutil.rmtree(\"./test\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6a4941f7-3283-4c18-9ff6-6255f2baec5e",
   "metadata": {},
   "source": [
    "## 4. Custom Dataset Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c80e2c9-78db-4050-b599-0d28fcf8d444",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
